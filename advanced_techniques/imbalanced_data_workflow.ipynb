{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imbalanced Data Workflow\n",
    "Imbalanced data is one of the most common (and most frustrating) issues encountered in machine learning, and I have developed a number of data augmentation techniques to help address it.\n",
    "\n",
    "Always the first step is to understand the data, but once exploration is complete we have to roll up our sleeves and get down to business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys; sys.path.append('./*'); sys.path.append('..')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "import matplotlib.pyplot as plt\n",
    "from os import path\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, WeightedRandomSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trainer import Trainer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "d = '../../../../ml_data/metal_sheet_data'\n",
    "dataset_d = path.join(d, 'preprocessed_training_set')\n",
    "\n",
    "from datasets.ClassificationSet import ClassificationSet\n",
    "from models.SheetClassifier import SheetClassifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Artificially Imbalanced Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-Cut Bubbles Len: 1693\n",
      "Post-Cut Bubbles Len: 338\n",
      "Number of bubbles compared to line: 0.2\n",
      "Number of bubbles compared to no defect: 0.21\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(f'{path.join(d, 'preprocessed_training_set')}/metadata.csv')\n",
    "bubbles = df[df.target==2]; print('Pre-Cut Bubbles Len:', len(bubbles))\n",
    "bubbles = bubbles.iloc[:int(len(bubbles)*.2)]\n",
    "\n",
    "print('Post-Cut Bubbles Len:',len(bubbles))\n",
    "print('Number of bubbles compared to line:', round(len(bubbles)/len(df[df.target==1]),2))\n",
    "print('Number of bubbles compared to no defect:', round(len(bubbles)/len(df[df.target==0]),2))\n",
    "df = pd.concat([df[df.target != 2], bubbles])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, so now our dataset is highly skewed against bubbles. Given that let's see how training goes without stratifying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Train Loss: 7.36, Accuracy: 0.46\n",
      "Epoch [6/10], Train Loss: 0.27, Accuracy: 0.92\n",
      "Experiment Complete\n",
      "[Evaluation over 8 Batches], Test Loss: 3.48, Accuracy: 0.50\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SheetClassifier(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lin1): Linear(in_features=16384, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "data = ClassificationSet(dataset_d, df)\n",
    "model = SheetClassifier()\n",
    "\n",
    "trainer.run_experiment(model, data, epochs=20, criterion=nn.CrossEntropyLoss, train_shuffle=False, show=True)\n",
    "trainer.evaluate_model(model, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_ds_split(df:pd.DataFrame, root_dir:str, dataset_class:type, target_column_names:list, reporting:bool = False) -> tuple:\n",
    "  ''' Stratifies the dataset by the given list of target columns, attempting to ensure consistent distribution of classes therein.\n",
    "      Allows for multiple columns to form the strata key (so if you want to group by age, sex, and whatever else, you can).\n",
    "  '''\n",
    "\n",
    "  # Ensures consistent distribution of the given classes\n",
    "  if len(target_column_names) == 1: stratification_key = target_column_names[0]\n",
    "  else:\n",
    "    stratification_key = 'strat_key'\n",
    "    df[stratification_key] = df[target_column_names].astype(str).sum(axis=1)\n",
    "\n",
    "  # Split the complete df into training and test\n",
    "  training_features, testing_features, training_targets, testing_targets = train_test_split(df, df[stratification_key], test_size=0.2, stratify=df[stratification_key])\n",
    "\n",
    "  # Generate weights for the classes based on their representation in the target series\n",
    "  target = np.array(training_features[stratification_key])\n",
    "  target_classes = np.unique(target)\n",
    "  weights = {t: len(np.where(target == t)[0])/len(training_features) for t in target_classes}\n",
    "  if reporting:\n",
    "    whole_df_target = np.array(df[stratification_key])\n",
    "    whole_df_weights = {t: len(np.where(whole_df_target == t)[0])/len(df) for t in target_classes}\n",
    "    print('Our target classes are as follows:',target_classes,'\\n','-'*50,\n",
    "                      '\\nTheir representation in the dataset as a whole are:\\n',{str(item[0]):round(float(item[1]),5) for item in whole_df_weights.items()},\n",
    "                      '\\nTheir representation in our training dataloader are:\\n',{str(item[0]):round(float(item[1]),5) for item in weights.items()})\n",
    "\n",
    "  weights_to_samples = np.array([weights[target[row_i]] for row_i in range(len(target))])\n",
    "  weights_to_samples = torch.from_numpy(weights_to_samples)\n",
    "  training_sampler = WeightedRandomSampler(weights_to_samples, len(weights_to_samples))\n",
    "\n",
    "  training_features['target'], testing_features['target'] = training_targets, testing_targets\n",
    "\n",
    "\n",
    "  return dataset_class(root_dir, training_features), dataset_class(root_dir, testing_features), training_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set, testing_set, train_y, test_y = train_test_split(df, df.target, test_size=0.2, stratify=df.target)\n",
    "# training_set.target = train_y; testing_set.target = test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our target classes are as follows: [0 1 2] \n",
      " -------------------------------------------------- \n",
      "Their representation in the dataset as a whole are:\n",
      " {'0': 0.43978, '1': 0.46749, '2': 0.09273} \n",
      "Their representation in our training dataloader are:\n",
      " {'0': 0.43964, '1': 0.46742, '2': 0.09294}\n"
     ]
    }
   ],
   "source": [
    "training_set, testing_set, sampler = stratified_ds_split(df, dataset_d, ClassificationSet, ['target'], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Train Loss: 4.47, Accuracy: 0.59\n",
      "Epoch [6/20], Train Loss: 0.27, Accuracy: 0.93\n",
      "Epoch [11/20], Train Loss: 0.09, Accuracy: 0.98\n",
      "Epoch [16/20], Train Loss: 0.10, Accuracy: 0.98\n",
      "Experiment Complete\n",
      "[Evaluation over 8 Batches], Test Loss: 1.93, Accuracy: 0.59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SheetClassifier(\n",
       "  (model): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): ReLU()\n",
       "    (5): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (lin1): Linear(in_features=16384, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "model = SheetClassifier()\n",
    "\n",
    "trainer.run_experiment(model, training_dataset=training_set, testing_dataset=testing_set, epochs=20, criterion=nn.CrossEntropyLoss,sampler=sampler, train_shuffle=False, show=True)\n",
    "trainer.evaluate_model(model, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "As can be seen, the stratified set performed about 9% better than the unstratified. These results are fairly underwhelming, so let's combine them with some data augmentation and then evaluate the resultant model on our original data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
